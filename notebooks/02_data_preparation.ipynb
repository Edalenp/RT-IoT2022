{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to drop (24): ['bwd_data_pkts_tot', 'bwd_pkts_per_sec', 'flow_pkts_per_sec', 'bwd_header_size_tot', 'bwd_header_size_max', 'flow_ACK_flag_count', 'bwd_pkts_payload.tot', 'bwd_pkts_payload.std', 'flow_pkts_payload.max', 'flow_pkts_payload.tot', 'fwd_iat.tot', 'fwd_iat.std', 'flow_iat.max', 'flow_iat.tot', 'flow_iat.std', 'payload_bytes_per_second', 'bwd_subflow_bytes', 'fwd_bulk_packets', 'bwd_bulk_packets', 'active.std', 'idle.min', 'idle.max', 'idle.tot', 'idle.avg']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv('../data/raw/train_data.csv')\n",
    "test_df = pd.read_csv('../data/raw/test_data.csv')\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = train_df.corr(numeric_only=True).abs()\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "print(f\"Features to drop ({len(to_drop)}): {to_drop}\")\n",
    "\n",
    "# Drop features\n",
    "train_df.drop(to_drop, axis=1, inplace=True)\n",
    "test_df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why remove features with correlation > 0.95?\n",
    "\n",
    "We remove features with a correlation higher than **0.95** to avoid **multicollinearity**. \n",
    "\n",
    "When two variables are highly correlated (almost identical), they provide redundant information to the model. Keeping both doesn't improve performance but increases computational cost and can make some models unstable. By removing one of them, we simplify the dataset without losing significant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (93568, 71), Test shape: (23393, 71)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "categorical_cols = ['proto', 'service']\n",
    "target_col = 'Attack_type'\n",
    "\n",
    "# One-Hot Encoding for features\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols)\n",
    "\n",
    "# Align columns to ensure train and test have same features\n",
    "train_df, test_df = train_df.align(test_df, join='inner', axis=1)\n",
    "\n",
    "# Label Encoding for target\n",
    "le = LabelEncoder()\n",
    "train_df[target_col] = le.fit_transform(train_df[target_col])\n",
    "test_df[target_col] = le.transform(test_df[target_col])\n",
    "\n",
    "# Scaling numeric features\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.drop(target_col)\n",
    "scaler = StandardScaler()\n",
    "train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
    "test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "# Combine train and test for splitting (80/20 split)\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "X = full_df.drop(target_col, axis=1)\n",
    "y = full_df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "# Reconstruct DataFrames for saving\n",
    "train_processed = pd.concat([X_train, y_train], axis=1)\n",
    "test_processed = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Save processed data\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "train_processed.to_csv('../data/processed/train_processed.csv', index=False)\n",
    "test_processed.to_csv('../data/processed/test_processed.csv', index=False)\n",
    "\n",
    "print(f\"Train shape: {train_processed.shape}, Test shape: {test_processed.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
